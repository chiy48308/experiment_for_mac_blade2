# 📊 Stack + System Scoring Integration Plan

## 🎯 研究目標

本研究旨在評估不同語音活動偵測（VAD）方法與特徵提取策略，對整體語音切割準確性與自動發音評分系統效能之影響，並將評分結果與 Azure 語音評分服務進行相關性分析，以佐證系統評分的外部效度。

---

## 🧱 Stack 架構總覽

每個 Stack 實驗單元包含：

1. **VAD 模型**：進行語音段落初步切割
2. **特徵提取方法**：計算語音區段特徵（如 MFCC, pitch, PLP）
3. **對齊方式**：與教師語音（標準音）對齊，確認切割準確性（MFA / DTW）
4. **語音評分模型**：使用回歸模型（如 Random Forest）產出語音分數
5. **外部驗證模組**：與 Azure Speech Scoring API 結果比較，計算相關性與誤差

---

## 🔁 Stack 實驗組別設計

| Stack 編號 | VAD 方法     | 特徵提取       | 對齊方式 | 評分模型      | 目的                           | 原實驗對應 |
|------------|--------------|----------------|----------|----------------|--------------------------------|------------|
| Stack #1   | WebRTC       | MFCC           | MFA      | RFRegressor     | 作為 baseline 對照             | 實驗一     |
| Stack #2   | Silero       | MFCC           | MFA      | RFRegressor     | 預訓練模型改善切割與評分精度   | 實驗二     |
| Stack #3   | Adaptive VAD | MFCC + PLP     | MFA + DTW| RFRegressor     | 適應性門檻 + 多特徵融合實驗     | 實驗三     |
| Stack #4   | QA-VAD       | MFCC + pitch   | MFA + DTW| RFRegressor     | 加入品質過濾，對抗低品質語音    | 新增實驗   |
| Stack #5   | -（無 VAD）   | -              | MFA      | -               | 對照組（理論上最佳切割基準）   | 新增基準組 |

實驗四重新定位為系統整合評估與Azure驗證，將評估所有Stack的端到端效能。

---

## 🔍 評估指標設計

### ✂️ 語音切割與對齊評估指標：

| 指標名稱        | 說明                                         |
|-----------------|----------------------------------------------|
| RMSE            | VAD 時間戳 vs. MFA 對齊的時間差異（秒）      |
| DTW 距離        | 學生特徵序列與教師特徵對齊誤差               |
| 切割段長偏差     | 平均語音段長與標準段長差異（過短/過長判斷） |
| 特徵保留率       | 切割後保留的有效語音特徵百分比               |
| 靜音誤檢率       | 靜音被錯誤標記為語音的比例                   |

### 📈 系統評分與 Azure 效度指標：

| 指標名稱         | 說明                                             |
|------------------|--------------------------------------------------|
| Pearson/Spearman r | 系統分數 vs Azure 分數之相關性                 |
| MAE（平均絕對誤差） | 預測分數與 Azure 分數差值的平均                 |
| 評分偏差 Bias     | 系統傾向過高或過低的趨勢                        |
| 散點圖與殘差圖     | 可視化評估系統分數與 Azure 差異的分布            |
| R² (決定係數)     | 模型解釋變異的能力                               |
| 分類一致性         | 高/中/低分類分布與Azure分類的一致程度            |

---

## 📁 輸入與輸出格式

### 輸入
- 語音檔案 `.wav`（16kHz採樣率、16bit深度）
- Azure API 回傳之語音評分 JSON
- 教師語音標準對齊時間（TextGrid 或 CSV）

### 輸出
- 每組 stack 的時間戳 JSON / CSV
- 特徵檔案（每段 MFCC / pitch / PLP）
- 系統預測分數 vs Azure 分數比較表格
- 分數相關性報表 + 視覺化圖表（散點圖、回歸圖）
- 特徵重要性（Feature Importance）報告

---

## 🔬 特徵提取與評分模型詳細說明

### 特徵提取方法

1. **MFCC (Mel-Frequency Cepstral Coefficients)**
   - 13維基本係數 + 一階差分 + 二階差分 = 39維特徵
   - 窗長25ms，步長10ms
   - 預加重係數0.97

2. **PLP (Perceptual Linear Prediction)**
   - 基於人類聽覺感知特性的線性預測係數
   - 與MFCC互補，提供不同角度的語音資訊

3. **Pitch特徵**
   - F0軌跡及其一階、二階差分
   - 基音穩定度（jitter, shimmer）
   - 諧噪比(HNR)

### 評分模型 - Random Forest Regressor

- **訓練策略**：K折交叉驗證（K=5）
- **超參數優化**：GridSearchCV + 隨機搜索
- **特徵選擇**：使用特徵重要性遞歸消除(RFE)
- **模型融合**：每個Stack層級的評分結果進行加權融合

---

## 🧪 實驗進行順序建議

1. **先處理高 SNR + 高 CV 的資料群**（數量最大、語音清晰）
2. 建立 baseline（WebRTC + MFCC + MFA + 評分模型）
3. 將 Stack 2~4 套入同一資料集進行比較
4. 最後將系統分數與 Azure 結果進行統計相關性分析
5. 擴展實驗至其他品質分區（低 SNR、高靜音比例等）

### 端到端流程設計

```
資料集選取 → VAD切割 → 特徵提取 → 對齊 → 評分模型訓練 → Azure比較
```

每個Stack實驗都完整執行以上流程，以評估該組合對最終評分的影響。

---

## 🔌 Azure API 整合與效度驗證

### Azure 語音評分服務整合

1. **API呼叫流程**
   - 批次處理各測試音檔
   - 獲取準確度、流暢度、完整度、韻律等分數
   - 保存完整JSON響應作為比對基準

2. **效度驗證方法**
   - 分數區間一致性分析
   - 等級分布比較（高/中/低三級分布比例）
   - 相關性分析（Pearson/Spearman）
   - Bland-Altman一致性圖

3. **偏差校正**
   - 系統內部校正模型（校準自建系統分數至Azure量尺）
   - 建立轉換函數，確保兩系統分數可比較

---

## 📌 預期成果

- 找出在語音評分任務中最有效的 VAD+特徵組合
- 證明自建系統與 Azure 分數有顯著正相關（例如 r > 0.75）
- 建立一套具備可信效度的自動語音評分流程
- 實現可擴充、模組化的 Stack 架構供後續擴展
- 提供不同語音品質條件下的最佳處理策略

---

## ✏️ 延伸應用方向（供未來研究）

- 自建語音標註系統 → 自我增強語音對齊與評分模型（semi-supervised）
- 使用 Azure 分數為 pseudo-label，進行 student-teacher 模型訓練
- 引入主觀人工評分（老師標註）作為第三組效標交叉驗證
- 整合深度學習方法（如 TDNN, CNN-LSTM）於特徵提取或VAD環節 